{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.pytorch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ================================================================== #\n",
    "#                     1. Basic autograd example 1                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.4775, -0.5440, -0.2857],\n",
      "        [-0.2160, -0.4123, -0.4933]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([0.2939, 0.0041], requires_grad=True)\n",
      "loss:  1.4591752290725708\n",
      "dL/dw:  tensor([[-0.8796, -0.9310,  0.4250],\n",
      "        [-0.3364, -0.2656, -0.1437]])\n",
      "dL/db:  tensor([ 0.6791, -0.2599])\n",
      "loss after 1 step optimization:  1.4338494539260864\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                    2. Basic autograd example 2                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                     3. Loading data from numpy                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                         4. Input pipeline                           #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                5. Input pipeline for custom dataset                 #\n",
    "# ================================================================== #\n",
    "\n",
    "# You should build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                        6. Pretrained model                         #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                      7. Save and load the model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.4103\n",
      "Epoch [10/60], Loss: 0.2698\n",
      "Epoch [15/60], Loss: 0.2129\n",
      "Epoch [20/60], Loss: 0.1898\n",
      "Epoch [25/60], Loss: 0.1805\n",
      "Epoch [30/60], Loss: 0.1767\n",
      "Epoch [35/60], Loss: 0.1751\n",
      "Epoch [40/60], Loss: 0.1745\n",
      "Epoch [45/60], Loss: 0.1742\n",
      "Epoch [50/60], Loss: 0.1741\n",
      "Epoch [55/60], Loss: 0.1740\n",
      "Epoch [60/60], Loss: 0.1740\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:05, 1767590.09it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 3297872.58it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:01, 1195553.60it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 5108191.36it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data\\MNIST\\raw\n",
      "\n",
      "Epoch [1/5], Step [100/600], Loss: 2.2558\n",
      "Epoch [1/5], Step [200/600], Loss: 2.1709\n",
      "Epoch [1/5], Step [300/600], Loss: 2.1255\n",
      "Epoch [1/5], Step [400/600], Loss: 2.0160\n",
      "Epoch [1/5], Step [500/600], Loss: 1.8949\n",
      "Epoch [1/5], Step [600/600], Loss: 1.8479\n",
      "Epoch [2/5], Step [100/600], Loss: 1.8067\n",
      "Epoch [2/5], Step [200/600], Loss: 1.6894\n",
      "Epoch [2/5], Step [300/600], Loss: 1.6039\n",
      "Epoch [2/5], Step [400/600], Loss: 1.5119\n",
      "Epoch [2/5], Step [500/600], Loss: 1.5618\n",
      "Epoch [2/5], Step [600/600], Loss: 1.4033\n",
      "Epoch [3/5], Step [100/600], Loss: 1.5691\n",
      "Epoch [3/5], Step [200/600], Loss: 1.4084\n",
      "Epoch [3/5], Step [300/600], Loss: 1.3278\n",
      "Epoch [3/5], Step [400/600], Loss: 1.3106\n",
      "Epoch [3/5], Step [500/600], Loss: 1.3007\n",
      "Epoch [3/5], Step [600/600], Loss: 1.1617\n",
      "Epoch [4/5], Step [100/600], Loss: 1.2283\n",
      "Epoch [4/5], Step [200/600], Loss: 1.1957\n",
      "Epoch [4/5], Step [300/600], Loss: 1.1669\n",
      "Epoch [4/5], Step [400/600], Loss: 1.0779\n",
      "Epoch [4/5], Step [500/600], Loss: 1.2271\n",
      "Epoch [4/5], Step [600/600], Loss: 1.2039\n",
      "Epoch [5/5], Step [100/600], Loss: 1.0281\n",
      "Epoch [5/5], Step [200/600], Loss: 1.0847\n",
      "Epoch [5/5], Step [300/600], Loss: 1.0389\n",
      "Epoch [5/5], Step [400/600], Loss: 1.0676\n",
      "Epoch [5/5], Step [500/600], Loss: 1.0377\n",
      "Epoch [5/5], Step [600/600], Loss: 1.0966\n",
      "Accuracy of the model on the 10000 test images: 82.22000122070312 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2865\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2676\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1818\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1383\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1201\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0716\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1153\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1720\n",
      "Epoch [2/5], Step [300/600], Loss: 0.2013\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1052\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0794\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1986\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0605\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0380\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1234\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0527\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0945\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0533\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0168\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1044\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0136\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0154\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0330\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0566\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0266\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0166\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1114\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0262\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0571\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0286\n",
      "Accuracy of the network on the 10000 test images: 98.04 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
